# -*- coding: utf-8 -*-
"""RonelAmarteifio._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1491i0BaZRn7Cg-E4tL_Wqkbu3HIxQDXm

FIFA Player Data Analysis and Modeling

Data Preprocessing
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer
import xgboost as xgb
import pickle
import joblib

from google.colab import drive
drive.mount('/content/drive')

#Inspecting Data
legacy_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/male_players (legacy).csv')
print(legacy_df.head())
print(legacy_df.describe())
print(legacy_df.info())

#Dropping Columns with More Than 30% Null Values
threshold = 0.3
total_rows = len(legacy_df)
legacy_df = legacy_df.loc[:, legacy_df.isnull().mean() < threshold]
print(f"Data shape after dropping columns with >{threshold*100}% null values: {legacy_df.shape}")

#Dropping Irrelevant Columns
useless_columns = ['player_id','player_url','player_face_url','short_name','long_name','fifa_version','fifa_update',
                   'fifa_update_date','dob','league_name','league_id','club_jersey_number','club_joined_date',
                   'club_contract_valid_until_year','nationality_id','nationality_name','preferred_foot',
                   'real_face','club_team_id','league_level']
legacy_df = legacy_df.drop(columns=useless_columns)
print("Remaining columns:", legacy_df.columns)

#Numeric and Categorical Data
numeric_data = legacy_df.select_dtypes(include=['int64', 'float64'])
categorical_data = legacy_df.select_dtypes(exclude=['int64', 'float64'])
print("Numeric data info:", numeric_data.info())
print("Textual data info:", categorical_data.info())

#Imputing Missing Values
numeric_data.fillna(numeric_data.mean(), inplace=True)

for column in categorical_data:
    categorical_data[column].fillna(categorical_data[column].mode()[0], inplace=True)

#Processing the 'gk' Column
# Replacing NaN with value
categorical_data['gk'].fillna('0+0', inplace=True)

# Splitting gk column
categorical_data[['gk_base', 'gk_modifier']] = categorical_data['gk'].str.split('+', expand=True)

#Adding the splits
categorical_data['gk_combined'] = categorical_data['gk_base'] + categorical_data['gk_modifier']

# Dropping the original column
categorical_data.drop(columns=['gk', 'gk_base', 'gk_modifier'], inplace=True)

#encode cateogrical data
label_encoder = LabelEncoder()
for column in categorical_data:
    categorical_data[column] = label_encoder.fit_transform(categorical_data[column])

#New and cleaned Dataframe
cleaned_legacy = pd.concat([numeric_data, categorical_data], axis=1)
print("Cleaned data shape:", cleaned_legacy.shape)

"""Feature Engineernig"""

X = cleaned_legacy.drop('overall', axis=1)
y = cleaned_legacy['overall']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Using Random Forest Clssifier to get feature importances
rforest_classifier = RandomForestClassifier(n_estimators=112, max_depth=12, criterion='entropy')
rforest_classifier.fit(X_train, y_train)
feature_importance = rforest_classifier.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
print(feature_importance_df.head(20))

#Top Correlated Features
selected_features = [
    'movement_reactions', 'potential', 'passing', 'wage_eur', 'value_eur',
    'dribbling', 'attacking_short_passing', 'international_reputation', 'skill_long_passing',
    'physic', 'age', 'skill_ball_control', 'shooting', 'skill_curve', 'weak_foot',
    'skill_moves', 'skill_dribbling', 'attacking_finishing'
]

# Standardize selected features
scaler = StandardScaler()
X = cleaned_legacy[selected_features]
X_scaled = scaler.fit_transform(X)
y = cleaned_legacy['overall']

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""Training And Evaluating Models"""

#Random Forest Regressor
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)
initial_predictions = model.predict(X_test)
initial_rmse = np.sqrt(mean_squared_error(y_test, initial_predictions))
print(f"Initial RMSE (RandomForest): {initial_rmse}")

# Hyperparameter tuning for RandomForestRegressor
param_grid = {
    'n_estimators': [100, 150, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
}
grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=4)
grid_search.fit(X_train, y_train)

#best model
best_model = grid_search.best_estimator_
rmse_scores = np.sqrt(-cross_val_score(best_model, X, y, scoring='neg_mean_squared_error', cv=5))
print(f"Mean RMSE (RandomForest with GridSearch): {np.mean(rmse_scores)}")

#XGBoost Model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_initial_predictions = xgb_model.predict(X_test)
xgb_initial_rmse = np.sqrt(mean_squared_error(y_test, xgb_initial_predictions))
print(f"Initial RMSE (XGBoost): {xgb_initial_rmse}")

# Hyperparameter tuning for XGBoost
param_grid_xgb = {
    'learning_rate': [0.1, 0.01],
    'n_estimators': [100, 500],
    'max_depth': [3, 5],
    'min_child_weight': [1, 3],
    'gamma': [0, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)
cv = KFold(n_splits=3, shuffle=True, random_state=42)
xgb_grid_search = GridSearchCV(xgb_model, param_grid=param_grid_xgb, scoring=scorer, cv=cv)
xgb_grid_search.fit(X_train, y_train)

#best XGBoost model
xgb_best_model = xgb_grid_search.best_estimator_
xgb_cv_scores = cross_val_score(xgb_best_model, X, y, cv=cv, scoring=scorer)
xgb_rmse_scores = -xgb_cv_scores
print(f"Mean RMSE (XGBoost with GridSearch): {np.mean(xgb_rmse_scores)}")

#Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()
gb_model.fit(X_train, y_train)
gb_initial_predictions = gb_model.predict(X_test)
gb_initial_rmse = np.sqrt(mean_squared_error(y_test, gb_initial_predictions))
print(f"Initial RMSE (GradientBoosting): {gb_initial_rmse}")

# Hyperparameter tuning
param_grid_gb = {
    'n_estimators': [150, 200],
    'learning_rate': [0.1, 0.2],
    'max_depth': [5, 7],
    'min_samples_split': [2, 5]
}
gb_grid_search = GridSearchCV(gb_model, param_grid_gb, scoring=scorer, cv=cv)
gb_grid_search.fit(X_train, y_train)

#best GradientBoosting model
gb_best_model = gb_grid_search.best_estimator_
gb_cv_scores = cross_val_score(gb_best_model, X, y, cv=cv, scoring=scorer)
gb_rmse_scores = -gb_cv_scores
print(f"Mean RMSE (GradientBoosting with GridSearch): {np.mean(gb_rmse_scores)}")

"""Testing and Deployment"""

#new dataset
players_22 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22-1.csv')

#Dropping Columns with More Than 30% Null Values
threshold = 0.3
total_rows = len(players_22)

players_22 = players_22.loc[:, players_22.isnull().mean() < threshold]
print(f"Data shape after dropping columns with >{threshold*100}% null values: {players_22.shape}")

# List of columns to drop
useless_columns22 = [
    'sofifa_id', 'player_url', 'long_name', 'dob', 'short_name', 'body_type', 'real_face',
    'player_face_url', 'club_logo_url', 'nation_flag_url', 'club_flag_url'
]
players_22 = players_22.drop(useless_columns22, axis=1)

#numeric and categorical columns
numeric_data1 = players_22.select_dtypes(include=['int64', 'float64'])
categorical_data1 = players_22.select_dtypes(exclude=['int64', 'float64'])

numeric_data1.info()
categorical_data1.info()

#Imputing Missing Values
numeric_data1.fillna(numeric_data1.mean(), inplace=True)

for column in categorical_data1:
    categorical_data1[column].fillna(categorical_data1[column].mode()[0], inplace=True)

#remaining null values
print(numeric_data1.isnull().sum())
print(categorical_data1.isnull().sum())

#Encoding
label_encoder = LabelEncoder()
for column in categorical_data1:
    categorical_data1[column] = label_encoder.fit_transform(categorical_data1[column])

#Combining Numeric and Categorical Data into one dataframe
new_players_22 = pd.concat([numeric_data1, categorical_data1], axis=1)
print("Cleaned data shape:", new_players_22.shape)

#Data Preparation for Prediction
selected_features = [
    'movement_reactions', 'potential', 'passing', 'wage_eur', 'value_eur',
    'dribbling', 'attacking_short_passing', 'international_reputation', 'skill_long_passing',
    'physic', 'age', 'skill_ball_control', 'shooting', 'skill_curve','weak_foot','skill_moves','skill_dribbling','attacking_finishing'
]
scaler = StandardScaler()
X = new_players_22[selected_features]
X_scaled = scaler.fit_transform(X)

# Define the directory to save the models
model_directory = '/content/drive/My Drive/Colab Notebooks/'

# Save the models
filename_rf = model_directory + 'random_forest_model.sav'
joblib.dump(best_model, filename_rf)

filename_xgb = model_directory + 'xgboost_model.sav'
joblib.dump(xgb_best_model, filename_xgb)

filename_gb = model_directory + 'gradient_boosting_model.sav'
joblib.dump(gb_best_model, filename_gb)

# Make predictions
rf_predictions = best_model.predict(X_scaled)
xgb_predictions = xgb_best_model.predict(X_scaled)
gb_predictions = gb_best_model.predict(X_scaled)

actual_data = new_players_22['overall']

rf_rmse_new = np.sqrt(mean_squared_error(actual_data, rf_predictions))
xgb_rmse_new = np.sqrt(mean_squared_error(actual_data, xgb_predictions))
gb_rmse_new = np.sqrt(mean_squared_error(actual_data, gb_predictions))

print("Random Forest RMSE on new data:", rf_rmse_new)
print("XGBoost RMSE on new data:", xgb_rmse_new)
print("Gradient Boosting RMSE on new data:", gb_rmse_new)